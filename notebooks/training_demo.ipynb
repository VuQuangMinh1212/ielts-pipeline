{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d82d1ef",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4f5a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332f7e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (if needed)\n",
    "# !pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09824182",
   "metadata": {},
   "source": [
    "## 2. Prepare Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a54e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from app.prepare_dataset import prepare_dataset, validate_dataset\n",
    "\n",
    "# Create sample dataset\n",
    "samples = prepare_dataset(\n",
    "    input_dir=\"../data/processed\",\n",
    "    output_file=\"../data/ielts_training_data.jsonl\"\n",
    ")\n",
    "\n",
    "print(f\"\\nCreated {len(samples)} training samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b8aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate dataset\n",
    "validate_dataset(\"../data/ielts_training_data.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5239c5ea",
   "metadata": {},
   "source": [
    "## 3. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4394c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.train_qlora import IELTSModelTrainer\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = IELTSModelTrainer(\n",
    "    model_name=\"qwen\",  # Options: qwen, phi2, gemma\n",
    "    output_dir=\"../models/ielts-demo\",\n",
    "    dataset_path=\"../data/ielts_training_data.jsonl\"\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6eeb96",
   "metadata": {},
   "source": [
    "## 4. Train Model with QLoRA\n",
    "\n",
    "**Note**: Training c√≥ th·ªÉ m·∫•t 10-30 ph√∫t t√πy v√†o dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d628d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "trained_model = trainer.train(\n",
    "    num_epochs=3,\n",
    "    batch_size=1,  # Small batch for 4GB GPU\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    max_seq_length=512,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf071c58",
   "metadata": {},
   "source": [
    "## 5. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cbef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.inference import AutotrainInferenceServer\n",
    "import json\n",
    "\n",
    "# Load trained model\n",
    "server = AutotrainInferenceServer(model_path=\"../models/ielts-demo\")\n",
    "\n",
    "print(\"‚úì Model loaded for inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2ca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample transcript\n",
    "test_transcript = \"\"\"\n",
    "Well, I think that technology has greatly influenced our daily lives. \n",
    "For example, smartphones allow us to stay connected with friends and family. \n",
    "Moreover, the internet provides us with access to vast amounts of information.\n",
    "\"\"\".strip()\n",
    "\n",
    "print(\"üìù Test Transcript:\")\n",
    "print(test_transcript)\n",
    "print(\"\\nüéØ Scoring...\")\n",
    "\n",
    "scores = server.score_transcript(test_transcript)\n",
    "print(json.dumps(scores, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d20b00",
   "metadata": {},
   "source": [
    "## 6. Convert to GGUF (Optional)\n",
    "\n",
    "Convert model to GGUF format for llama.cpp CPU inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd5929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.convert_model import convert_to_gguf\n",
    "\n",
    "# Convert to GGUF\n",
    "gguf_path = convert_to_gguf(\n",
    "    model_path=\"../models/ielts-demo\",\n",
    "    output_path=\"../models/ielts-demo/model.gguf\",\n",
    "    quantization=\"Q4_K_M\"  # 4-bit quantization\n",
    ")\n",
    "\n",
    "if gguf_path:\n",
    "    print(f\"‚úì Model converted to {gguf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb2e2a7",
   "metadata": {},
   "source": [
    "## 7. Benchmark Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2dbe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Benchmark inference speed\n",
    "test_cases = [\n",
    "    \"I like technology.\",\n",
    "    \"Well, I think education is important for personal development.\",\n",
    "    \"In my opinion, environmental protection requires collective effort from all members of society.\"\n",
    "]\n",
    "\n",
    "print(\"üìä Benchmarking inference speed...\\n\")\n",
    "\n",
    "for i, transcript in enumerate(test_cases, 1):\n",
    "    start = time.time()\n",
    "    scores = server.score_transcript(transcript)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"Test {i}: {elapsed*1000:.2f}ms\")\n",
    "    print(f\"  Transcript length: {len(transcript.split())} words\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51687b8",
   "metadata": {},
   "source": [
    "## 8. Compare Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0afad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and compare different base models\n",
    "models_to_test = [\"qwen\", \"phi2\", \"gemma\"]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Testing {model_name.upper()}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    # Note: This will take a while!\n",
    "    # trainer = IELTSModelTrainer(\n",
    "    #     model_name=model_name,\n",
    "    #     output_dir=f\"../models/ielts-{model_name}\",\n",
    "    # )\n",
    "    # trainer.train(num_epochs=1, batch_size=1)\n",
    "    \n",
    "    print(f\"‚úì {model_name} training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f705b5a",
   "metadata": {},
   "source": [
    "## 9. Export for Production\n",
    "\n",
    "Final checklist before deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f354b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "model_dir = Path(\"../models/ielts-demo\")\n",
    "\n",
    "# Check required files\n",
    "required_files = [\n",
    "    \"config.json\",\n",
    "    \"adapter_config.json\",\n",
    "    \"adapter_model.safetensors\",\n",
    "    \"tokenizer.json\",\n",
    "    \"tokenizer_config.json\",\n",
    "]\n",
    "\n",
    "print(\"üì¶ Deployment Checklist:\\n\")\n",
    "\n",
    "for file in required_files:\n",
    "    exists = (model_dir / file).exists()\n",
    "    status = \"‚úì\" if exists else \"‚úó\"\n",
    "    print(f\"{status} {file}\")\n",
    "\n",
    "print(\"\\n‚úì Model ready for deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f8eb11",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. ‚úÖ Model ƒë√£ ƒë∆∞·ª£c train v·ªõi QLoRA\n",
    "2. ‚úÖ Test inference th√†nh c√¥ng\n",
    "3. üîÑ Deploy v·ªõi vLLM ho·∫∑c llama.cpp\n",
    "4. üîÑ Integrate v√†o production API\n",
    "5. üîÑ Monitor performance v√† fine-tune th√™m\n",
    "\n",
    "Xem th√™m:\n",
    "- [Training Guide](../docs/TRAINING_DEPLOYMENT.md)\n",
    "- [Quick Start](../docs/QUICKSTART.md)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
