# CPU-only Dockerfile for llama.cpp inference
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    cmake \
    git \
    && rm -rf /var/lib/apt/lists/*

# Install llama-cpp-python
RUN pip install --no-cache-dir llama-cpp-python

# Copy application
COPY . .

EXPOSE 8002

# Start llama.cpp server
CMD ["python3", "-u", "app/inference.py", "--backend", "llamacpp", "--cpu"]
